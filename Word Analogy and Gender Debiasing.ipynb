{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word analogy task\n",
    "\n",
    "In this notebook, I implement the word analogy task using a pretrained set of word to vector embeddings.\n",
    "\n",
    "In the word analogy task, our goal is to complete the sentence *a* is to *b* as *c* is to **____**. For example, '*boy* is to *girl* as *king* is to *queen*'.\n",
    "\n",
    "Technically, we are trying to find a word *d* for a set of words *a*, *b* and *c* such that the difference in the word embeddings of *a* and *b* is equal to that of *c* and *d*.\n",
    "\n",
    "Let the vectors embeddings be $e_a, e_b, e_c, e_d$ and we will measure the similarity between  $e_b - e_a$ and $e_d - e_c$\n",
    "using cosine similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity between two vectors is defined as follows: \n",
    "\n",
    "$$\\text{CosineSimilarity(x, y)} = \\frac {x . y} {||x||_2 ||y||_2} = cos(\\theta) $$\n",
    "\n",
    "If $x$ and $y$ are very similar, their cosine similarity will be close to 1; if they are dissimilar, the cosine similarity will take a smaller value. \n",
    "\n",
    "Lets load the packages we will need below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from w2v_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the pre-trained word to vector embeddings by Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to compute cosine similarity as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similariy between x and y\n",
    "        \n",
    "    Arguments:\n",
    "        x -- a word vector of shape (n,)          \n",
    "        y -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between x and y defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0.0\n",
    "    \n",
    "    # Compute the dot product between x and y (≈1 line)\n",
    "    dot = np.dot(x,y)\n",
    "    # Compute the L2 norm of x (≈1 line)\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    \n",
    "    # Compute the L2 norm of y (≈1 line)\n",
    "    norm_y = np.linalg.norm(y)\n",
    "    # Compute the cosine similarity defined by formula (1) (≈1 line)\n",
    "    cosine_similarity = dot / (norm_x * norm_y)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the function to compute word analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_a -- a word, string\n",
    "    word_b -- a word, string\n",
    "    word_c -- a word, string\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert words to lower case\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    # Get the word embeddings v_a, v_b and v_c \n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a],word_to_vec_map[word_b],word_to_vec_map[word_c]\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "\n",
    "    # loop over the whole word vector set\n",
    "    for w in words:        \n",
    "        # to avoid best_word being one of the input words, pass on them.\n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  \n",
    "        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word \n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your code, this may take 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: spain -> spanish\n",
      "india -> delhi :: japan -> tokyo\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: large -> larger\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try more triads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "husband is to wife as prince is to duchess\n",
      "America is to Chicago as Canada is to toronto\n",
      "small is to large as tiny is to surpluses\n"
     ]
    }
   ],
   "source": [
    "triads = [('husband', 'wife','prince'),('America','Chicago','Canada'),('small','large','tiny')]\n",
    "for triad1 in triads:\n",
    "    print('{} is to {} as {} is to {}'.format(*triad1, complete_analogy(*triad1, word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Debiasing word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first see how the GloVe word embeddings relate to gender. You will first compute a vector $g = e_{woman}-e_{man}$, where $e_{woman}$ represents the word vector corresponding to the word *woman*, and $e_{man}$ corresponds to the word vector corresponding to the word *man*. The resulting vector $g$ roughly encodes the concept of \"gender\".\n",
    "\n",
    "We compute  $g1 = e_{woman}-e_{man}$, $g_2 = e_{mother}-e_{father}$, $g_3 = e_{girl}-e_{boy}$ and average over them and call it $g$. Thus $g$ roughly represents the concept of \"female gender\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "g1 = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "g2 = word_to_vec_map['mother'] - word_to_vec_map['father']\n",
    "g3 = word_to_vec_map['girl'] - word_to_vec_map['boy']\n",
    "g = (g1 + g2+g3 )/ 3\n",
    "print(g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how $g$ compares with some masculine, feminine or profession related words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john -0.30873091089769905\n",
      "marie 0.34257515107827113\n",
      "sophie 0.4116200252265308\n",
      "ronaldo -0.29083978511732383\n",
      "priya 0.1964679344860046\n",
      "rahul -0.1949214763863341\n",
      "danielle 0.2923957653171285\n",
      "reza -0.1679382162425299\n",
      "katy 0.31132430605664346\n",
      "yasmin 0.19658379893678699\n",
      "lipstick 0.4136681512625245\n",
      "guns -0.08755154639507809\n",
      "science -0.058374259643848236\n",
      "arts 0.011760468125783748\n",
      "literature 0.023169467893751714\n",
      "warrior -0.1656463810030795\n",
      "doctor 0.0772141272665668\n",
      "tree 0.03538042107098229\n",
      "receptionist 0.30167259871100655\n",
      "technology -0.1619210846255818\n",
      "fashion 0.1416547219136271\n",
      "teacher 0.10545901736578715\n",
      "engineer -0.22639944157426764\n",
      "pilot -0.03699357317847416\n",
      "computer -0.16821031921735138\n",
      "singer 0.2009300079322624\n"
     ]
    }
   ],
   "source": [
    "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin','lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that female names have positive cosine similarity while male names have negative cosine similarity with $g$. But it can also be observed that words like \"technology\", \"engineer\" and \"computer\" have negative cosine similarity with $g$ while words like \"receptionst\" has positive cos. similarity with $g$. Hence there is a gender bias in these vectors and we need to reduce it. We will use the alogorithm by  [Boliukbasi et al., 2016](https://arxiv.org/abs/1607.06520).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To neutralise any embedding vector $e$, we first find the component of $e$ in the direction of $g$ and then remove it from $e$ as below,\n",
    "\n",
    "$$e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g$$\n",
    "$$e^{debiased} = e - e^{bias\\_component}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word, g, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias axis. \n",
    "    This function ensures that gender neutral words are zero in the gender subspace.\n",
    "    \n",
    "    Arguments:\n",
    "        word -- string indicating the word to debias\n",
    "        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)\n",
    "        word_to_vec_map -- dictionary mapping words to their corresponding vectors.\n",
    "    \n",
    "    Returns:\n",
    "        e_debiased -- neutralized word vector representation of the input \"word\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select word vector representation of \"word\". Use word_to_vec_map. (≈ 1 line)\n",
    "    e = word_to_vec_map[word]\n",
    "    \n",
    "    # Compute e_biascomponent using the formula give above. (≈ 1 line)\n",
    "    e_biascomponent = (np.dot(e,g)/ ((np.linalg.norm(g))**2)) * g\n",
    " \n",
    "    # Neutralize e by substracting e_biascomponent from it \n",
    "    # e_debiased should be equal to its orthogonal projection. (≈ 1 line)\n",
    "    e_debiased = e - e_biascomponent\n",
    "    \n",
    "    return e_debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between science and g, before neutralizing:  -0.058374259643848236\n",
      "cosine similarity between science and g, after neutralizing:  0.0\n",
      "\n",
      "\n",
      "cosine similarity between receptionist and g, before neutralizing:  0.30167259871100655\n",
      "cosine similarity between receptionist and g, after neutralizing:  0.0\n"
     ]
    }
   ],
   "source": [
    "e = \"science\"\n",
    "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"science\"], g))\n",
    "\n",
    "e_debiased = neutralize(\"science\", g, word_to_vec_map)\n",
    "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased, g))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "e = \"receptionist\"\n",
    "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"receptionist\"], g))\n",
    "\n",
    "e_debiased = neutralize(\"science\", g, word_to_vec_map)\n",
    "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased, g))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalization algorithm for gender-specific words\n",
    "\n",
    "By equalization, we want to make sure that words like \"babysit\" are equidistant from masculine and feminine words like \"husband\" or \"wife\". Technically, we want to make the vector embedding for \"babysit\" to be equidistant from the 49-dimesional $g_\\perp$ ($g_\\perp$ is orthogonal to $g$).\n",
    "\n",
    "We achieve this with the formulae below (See Bolukbasi et al., 2016 for details.):\n",
    "\n",
    "$$ \\mu = \\frac{e_{w1} + e_{w2}}{2}$$ \n",
    "\n",
    "$$ \\mu_{B} = \\frac {\\mu \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}$$ \n",
    "\n",
    "$$\\mu_{\\perp} = \\mu - \\mu_{B} $$\n",
    "\n",
    "$$ e_{w1B} = \\frac {e_{w1} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}$$ \n",
    "$$ e_{w2B} = \\frac {e_{w2} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}$$\n",
    "\n",
    "\n",
    "$$e_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w1B}} - \\mu_B} {|(e_{w1} - \\mu_{\\perp}) - \\mu_B)|} $$\n",
    "\n",
    "\n",
    "$$e_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w2B}} - \\mu_B} {|(e_{w2} - \\mu_{\\perp}) - \\mu_B)|} $$\n",
    "\n",
    "$$e_1 = e_{w1B}^{corrected} + \\mu_{\\perp} $$\n",
    "$$e_2 = e_{w2B}^{corrected} + \\mu_{\\perp} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(pair, bias_axis, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Debias gender specific words by following the equalize method described in the figure above.\n",
    "    \n",
    "    Arguments:\n",
    "    pair -- pair of strings of gender specific words to debias, e.g. (\"actress\", \"actor\") \n",
    "    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
    "    word_to_vec_map -- dictionary mapping words to their corresponding vectors\n",
    "    \n",
    "    Returns\n",
    "    e_1 -- word vector corresponding to the first word\n",
    "    e_2 -- word vector corresponding to the second word\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Select word vector representation of \"word\". Use word_to_vec_map. (≈ 2 lines)\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2] \n",
    "    \n",
    "    # Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)\n",
    "    mu = (e_w1 + e_w2) / 2\n",
    "\n",
    "    # Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)\n",
    "    mu_B = (np.dot(mu , bias_axis )/ ((np.linalg.norm( bias_axis ))**2)) * bias_axis\n",
    "    mu_orth = mu - mu_B\n",
    "\n",
    "    # Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines)\n",
    "    e_w1B = (np.dot(e_w1 , bias_axis )/ ((np.linalg.norm( bias_axis ))**2)) * bias_axis\n",
    "    e_w2B = (np.dot(e_w2 , bias_axis )/ ((np.linalg.norm( bias_axis ))**2)) * bias_axis\n",
    "        \n",
    "    # Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines)\n",
    "    corrected_e_w1B = np.sqrt( np.abs( 1 - (np.linalg.norm (mu_orth))**2  )) * (e_w1B - mu_B ) / (np.abs ( (e_w1 - mu_orth) - mu_B))\n",
    "    corrected_e_w2B = np.sqrt( np.abs( 1 - (np.linalg.norm (mu_orth))**2  )) * (e_w2B - mu_B ) / (np.abs ( (e_w2 - mu_orth) - mu_B))\n",
    "\n",
    "    # Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)\n",
    "    e1 = corrected_e_w1B + mu_orth\n",
    "    e2 = corrected_e_w2B + mu_orth\n",
    "                                                                \n",
    "    \n",
    "    return e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n",
      "cosine_similarity(word_to_vec_map[\"husband\"], gender) =  0.19353194031263787\n",
      "cosine_similarity(word_to_vec_map[\"wife\"], gender) =  0.31026048013658003\n",
      "\n",
      "\n",
      "cosine similarities after equalizing:\n",
      "cosine_similarity(e1, gender) =  -0.6637685201826582\n",
      "cosine_similarity(e2, gender) =  0.6626334732252718\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"husband\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"husband\"], g))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"wife\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"wife\"], g))\n",
    "print('\\n')\n",
    "e1, e2 = equalize((\"husband\", \"wife\"), g, word_to_vec_map)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this way we achieve gender debiasing and equalization. We don't need to perform the neutralization operation **neutralize** on the vector separately as the function **equalize** takes care of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "- The debiasing algorithm is from Bolukbasi et al., 2016, [Man is to Computer Programmer as Woman is to\n",
    "Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n",
    "- The GloVe word embeddings were due to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "8hb5s",
   "launcher_item_id": "5NrJ6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
